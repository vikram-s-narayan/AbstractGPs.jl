var documenterSearchIndex = {"docs":
[{"location":"api/#API","page":"API","title":"API","text":"","category":"section"},{"location":"api/","page":"API","title":"API","text":"Modules = [AbstractGPs]","category":"page"},{"location":"api/#AbstractGPs.AbstractGP","page":"API","title":"AbstractGPs.AbstractGP","text":"abstract type AbstractGP end\n\nSupertype for various Gaussian process (GP) types. A common interface is provided for interacting with each of these objects. See [1] for an overview of GPs.\n\n[1] - C. E. Rasmussen and C. Williams. \"Gaussian processes for machine learning\".  MIT Press. 2006.\n\n\n\n\n\n","category":"type"},{"location":"api/#AbstractGPs.ConstMean","page":"API","title":"AbstractGPs.ConstMean","text":"ConstMean{T<:Real} <: MeanFunction\n\nReturns c everywhere.\n\n\n\n\n\n","category":"type"},{"location":"api/#AbstractGPs.CustomMean","page":"API","title":"AbstractGPs.CustomMean","text":"CustomMean{Tf} <: MeanFunction\n\nA wrapper around whatever unary function you fancy. Must be able to be mapped over an AbstractVector of inputs.\n\n\n\n\n\n","category":"type"},{"location":"api/#AbstractGPs.FiniteGP","page":"API","title":"AbstractGPs.FiniteGP","text":"FiniteGP{Tf<:AbstractGP, Tx<:AbstractVector, TΣy}\n\nThe finite-dimensional projection of the AbstractGP f at x. Assumed to be observed under Gaussian noise with zero mean and covariance matrix Σ\n\n\n\n\n\n","category":"type"},{"location":"api/#AbstractGPs.GP","page":"API","title":"AbstractGPs.GP","text":"GP{Tm<:MeanFunction, Tk<:Kernel}\n\nA Gaussian Process (GP) with known mean and kernel. See e.g. [1] for an introduction.\n\nZero Mean\n\nIf only one argument is provided, assume the mean to be zero everywhere:\n\njulia> f = GP(Matern32Kernel());\n\njulia> x = randn(5);\n\njulia> mean(f(x)) == zeros(5)\ntrue\n\njulia> cov(f(x)) == kernelmatrix(Matern32Kernel(), x)\ntrue\n\nConstant Mean\n\nIf a Real is provided as the first argument, assume the mean function is constant with that value\n\njulia> f = GP(5.0, Matern32Kernel());\n\njulia> x = randn(5);\n\njulia> mean(f(x)) == 5.0 .* ones(5)\ntrue\n\njulia> cov(f(x)) == kernelmatrix(Matern32Kernel(), x)\ntrue\n\nCustom Mean\n\nProvide an arbitrary function to compute the mean:\n\njulia> f = GP(x -> sin(x) + cos(x / 2), Matern32Kernel());\n\njulia> x = randn(5);\n\njulia> mean(f(x)) == sin.(x) .+ cos.(x ./ 2)\ntrue\n\njulia> cov(f(x)) == kernelmatrix(Matern32Kernel(), x)\ntrue\n\n[1] - C. E. Rasmussen and C. Williams. \"Gaussian processes for machine learning\".  MIT Press. 2006.\n\n\n\n\n\n","category":"type"},{"location":"api/#AbstractGPs.LatentFiniteGP","page":"API","title":"AbstractGPs.LatentFiniteGP","text":"LatentFiniteGP(fx<:FiniteGP, lik)\n\nfx is a FiniteGP.\nlik is the log likelihood function which maps sample from f to corresposing \n\nconditional likelihood distributions.\n\n\n\n\n\n","category":"type"},{"location":"api/#AbstractGPs.LatentGP","page":"API","title":"AbstractGPs.LatentGP","text":"LatentGP(f<:GP, lik, Σy)\n\nf is a AbstractGP.\nlik is the log likelihood function which maps sample from f to corresposing \n\nconditional likelihood distributions.\n\nΣy is the observation noise\n\n\n\n\n\n","category":"type"},{"location":"api/#AbstractGPs.ZeroMean","page":"API","title":"AbstractGPs.ZeroMean","text":"ZeroMean{T<:Real} <: MeanFunction\n\nReturns zero(T) everywhere.\n\n\n\n\n\n","category":"type"},{"location":"api/#AbstractGPs.approx_posterior-Tuple{VFE,AbstractGPs.FiniteGP,AbstractArray{var\"#s16\",1} where var\"#s16\"<:Real,AbstractGPs.FiniteGP}","page":"API","title":"AbstractGPs.approx_posterior","text":"approx_posterior(::VFE, fx::FiniteGP, y::AbstractVector{<:Real}, u::FiniteGP)\n\nCompute the optimal approximate posterior [1] over the process f, given observations y of f at x, and inducing points u, where u = f(z) for some inducing inputs z.\n\n[1] - M. K. Titsias. \"Variational learning of inducing variables in sparse Gaussian processes\". In: Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics. 2009.\n\n\n\n\n\n","category":"method"},{"location":"api/#AbstractGPs.cov_diag-Tuple{AbstractGPs.AbstractGP,AbstractArray{T,1} where T}","page":"API","title":"AbstractGPs.cov_diag","text":"cov_diag(f::AbstractGP, x::AbstractVector)\n\nCompute only the diagonal elements of cov(f(x)).\n\n\n\n\n\n","category":"method"},{"location":"api/#AbstractGPs.dtc-Tuple{AbstractGPs.FiniteGP,AbstractArray{var\"#s15\",1} where var\"#s15\"<:Real,AbstractGPs.FiniteGP}","page":"API","title":"AbstractGPs.dtc","text":"dtc(f::FiniteGP, y::AbstractVector{<:Real}, u::FiniteGP)\n\nThe Deterministic Training Conditional (DTC) [1]. y are observations of f, and u are pseudo-points.\n\njulia> f = GP(Matern52Kernel());\n\njulia> x = randn(1000);\n\njulia> z = range(-5.0, 5.0; length=256);\n\njulia> y = rand(f(x, 0.1));\n\njulia> isapprox(dtc(f(x, 0.1), y, f(z)), logpdf(f(x, 0.1), y); atol=1e-3, rtol=1e-3)\ntrue\n\n[1] - M. Seeger, C. K. I. Williams and N. D. Lawrence. \"Fast Forward Selection to Speed Up Sparse Gaussian Process Regression\". In: Proceedings of the Ninth International Workshop on Artificial Intelligence and Statistics. 2003\n\n\n\n\n\n","category":"method"},{"location":"api/#AbstractGPs.elbo-Tuple{AbstractGPs.FiniteGP,AbstractArray{var\"#s13\",1} where var\"#s13\"<:Real,AbstractGPs.FiniteGP}","page":"API","title":"AbstractGPs.elbo","text":"elbo(f::FiniteGP, y::AbstractVector{<:Real}, u::FiniteGP)\n\nThe Titsias Evidence Lower BOund (ELBO) [1]. y are observations of f, and u are pseudo-points, where u = f(z) for some z.\n\njulia> f = GP(Matern52Kernel());\n\njulia> x = randn(1000);\n\njulia> z = range(-5.0, 5.0; length=13);\n\njulia> y = rand(f(x, 0.1));\n\njulia> elbo(f(x, 0.1), y, f(z)) < logpdf(f(x, 0.1), y)\ntrue\n\n[1] - M. K. Titsias. \"Variational learning of inducing variables in sparse Gaussian processes\". In: Proceedings of the Twelfth International Conference on Artificial Intelligence and Statistics. 2009.\n\n\n\n\n\n","category":"method"},{"location":"api/#AbstractGPs.marginals-Tuple{AbstractGPs.FiniteGP}","page":"API","title":"AbstractGPs.marginals","text":"marginals(f::FiniteGP)\n\nCompute a vector of Normal distributions representing the marginals of f efficiently. In particular, the off-diagonal elements of cov(f(x)) are never computed.\n\njulia> f = GP(Matern32Kernel());\n\njulia> x = randn(11);\n\njulia> fs = marginals(f(x));\n\njulia> mean.(fs) == mean(f(x))\ntrue\n\njulia> std.(fs) == sqrt.(diag(cov(f(x))))\ntrue\n\n\n\n\n\n","category":"method"},{"location":"api/#AbstractGPs.mean_and_cov-Tuple{AbstractGPs.AbstractGP,AbstractArray{T,1} where T}","page":"API","title":"AbstractGPs.mean_and_cov","text":"mean_and_cov(f::AbstractGP, x::AbstractVector)\n\nCompute both mean(f(x)) and cov(f(x)). Sometimes more efficient than separately computation, particularly for posteriors.\n\n\n\n\n\n","category":"method"},{"location":"api/#AbstractGPs.mean_and_cov-Tuple{AbstractGPs.FiniteGP}","page":"API","title":"AbstractGPs.mean_and_cov","text":"mean_and_cov(f::FiniteGP)\n\nEquivalent to (mean(f), cov(f)), but sometimes more efficient to compute them jointly than separately.\n\njulia> fx = GP(SqExponentialKernel())(range(-3.0, 3.0; length=10), 0.1);\n\njulia> mean_and_cov(fx) == (mean(fx), cov(fx))\ntrue\n\n\n\n\n\n","category":"method"},{"location":"api/#AbstractGPs.mean_and_cov_diag-Tuple{AbstractGPs.AbstractGP,AbstractArray{T,1} where T}","page":"API","title":"AbstractGPs.mean_and_cov_diag","text":"mean_and_cov_diag(f::AbstractGP, x::AbstractVector)\n\nCompute both mean(f(x)) and the diagonal elements of cov(f(x)). Sometimes more efficient than separately computation, particularly for posteriors.\n\n\n\n\n\n","category":"method"},{"location":"api/#AbstractGPs.posterior-Tuple{AbstractGPs.FiniteGP,AbstractArray{var\"#s13\",1} where var\"#s13\"<:Real}","page":"API","title":"AbstractGPs.posterior","text":"posterior(fx::FiniteGP, y::AbstractVector{<:Real})\n\nConstructs the posterior distribution over fx.f given observations y at x made under noise fx.Σy. This is another AbstractGP object. See chapter 2 of [1] for a recap on exact inference in GPs. This posterior process has mean function\n\nm_posterior(x) = m(x) + k(x, fx.x) inv(cov(fx)) (y - mean(fx))\n\nand kernel\n\nk_posterior(x, z) = k(x, z) - k(x, fx.x) inv(cov(fx)) k(fx.x, z)\n\nwhere m and k are the mean function and kernel of fx.f respectively.\n\n\n\n\n\n","category":"method"},{"location":"api/#AbstractGPs.posterior-Tuple{AbstractGPs.FiniteGP{var\"#s13\",Tx,TΣ} where TΣ where Tx<:(AbstractArray{T,1} where T) where var\"#s13\"<:AbstractGPs.PosteriorGP,AbstractArray{var\"#s23\",1} where var\"#s23\"<:Real}","page":"API","title":"AbstractGPs.posterior","text":"posterior(fx::FiniteGP{<:PosteriorGP}, y::AbstractVector{<:Real})\n\nConstructs the posterior distribution over fx.f when f is itself a PosteriorGP by updating the cholesky factorisation of the covariance matrix and avoiding recomputing it from original covariance matrix. It does this by using update_chol functionality.\n\nOther aspects are similar to a regular posterior.\n\n\n\n\n\n","category":"method"},{"location":"api/#AbstractGPs.sampleplot!-Tuple","page":"API","title":"AbstractGPs.sampleplot!","text":"sampleplot(GP::FiniteGP, samples)\n\nPlot samples from the given FiniteGP. Make sure to run using Plots before using this  function. \n\nExample\n\nusing Plots\nf = GP(SqExponentialKernel())\nsampleplot(f(rand(10)), 10; markersize=5)\n\nThe given example plots 10 samples from the given FiniteGP. The markersize is modified from default of 0.5 to 5.\n\n\n\n\n\n","category":"method"},{"location":"api/#AbstractGPs.sampleplot!-Tuple{RecipesBase.AbstractPlot,Vararg{Any,N} where N}","page":"API","title":"AbstractGPs.sampleplot!","text":"sampleplot(GP::FiniteGP, samples)\n\nPlot samples from the given FiniteGP. Make sure to run using Plots before using this  function. \n\nExample\n\nusing Plots\nf = GP(SqExponentialKernel())\nsampleplot(f(rand(10)), 10; markersize=5)\n\nThe given example plots 10 samples from the given FiniteGP. The markersize is modified from default of 0.5 to 5.\n\n\n\n\n\n","category":"method"},{"location":"api/#AbstractGPs.sampleplot-Tuple","page":"API","title":"AbstractGPs.sampleplot","text":"sampleplot(GP::FiniteGP, samples)\n\nPlot samples from the given FiniteGP. Make sure to run using Plots before using this  function. \n\nExample\n\nusing Plots\nf = GP(SqExponentialKernel())\nsampleplot(f(rand(10)), 10; markersize=5)\n\nThe given example plots 10 samples from the given FiniteGP. The markersize is modified from default of 0.5 to 5.\n\n\n\n\n\n","category":"method"},{"location":"api/#AbstractGPs.update_approx_posterior-Tuple{AbstractGPs.ApproxPosteriorGP,AbstractGPs.FiniteGP,AbstractArray{var\"#s15\",1} where var\"#s15\"<:Real}","page":"API","title":"AbstractGPs.update_approx_posterior","text":"function update_approx_posterior(\n    f_post_approx::ApproxPosteriorGP,\n    fx::FiniteGP,\n    y::AbstractVector{<:Real}\n)\n\nUpdate the ApproxPosteriorGP given a new set of observations. Here, we retain the same  of pseudo-points.\n\n\n\n\n\n","category":"method"},{"location":"api/#AbstractGPs.update_approx_posterior-Tuple{AbstractGPs.ApproxPosteriorGP,AbstractGPs.FiniteGP}","page":"API","title":"AbstractGPs.update_approx_posterior","text":"function update_approx_posterior(\n    f_post_approx::ApproxPosteriorGP,\n    u::FiniteGP,\n)\n\nUpdate the ApproxPosteriorGP given a new set of pseudo-points to append to the existing  set of pseudo points. \n\n\n\n\n\n","category":"method"},{"location":"api/#AbstractGPs.update_chol-Tuple{LinearAlgebra.Cholesky,AbstractArray{T,2} where T,AbstractArray{T,2} where T}","page":"API","title":"AbstractGPs.update_chol","text":" update_chol(chol::Cholesky, C12::AbstractMatrix, C22::AbstractMatrix)\n\nLet C be the positive definite matrix comprising blocks\n\nC = [C11 C12;\n     C21 C22]\n\nwith upper-triangular cholesky factorisation comprising blocks\n\nU = [U11 U12;\n     0   U22]\n\nwhere U11 and U22 are themselves upper-triangular, and U11 = cholesky(C11).U. update_chol computes the updated Cholesky given original chol, C12, and C22.\n\nArguments\n\n - chol::Cholesky: The original cholesky decomposition\n - C12::AbstractMatrix: matrix of size (size(chol.U, 1), size(C22, 1))\n - C22::AbstractMatrix: positive-definite matrix\n\n\n\n\n\n","category":"method"},{"location":"api/#Base.rand-Tuple{Random.AbstractRNG,AbstractGPs.FiniteGP,Int64}","page":"API","title":"Base.rand","text":"rand(rng::AbstractRNG, f::FiniteGP, N::Int=1)\n\nObtain N independent samples from the marginals f using rng. Single-sample methods produce a length(f) vector. Multi-sample methods produce a length(f) x N Matrix.\n\njulia> f = GP(Matern32Kernel());\n\njulia> x = randn(11);\n\njulia> rand(f(x)) isa Vector{Float64}\ntrue\n\njulia> rand(MersenneTwister(123456), f(x)) isa Vector{Float64}\ntrue\n\njulia> rand(f(x), 3) isa Matrix{Float64}\ntrue\n\njulia> rand(MersenneTwister(123456), f(x), 3) isa Matrix{Float64}\ntrue\n\n\n\n\n\n","category":"method"},{"location":"api/#Distributions.logpdf-Tuple{AbstractGPs.FiniteGP,Union{AbstractArray{var\"#s16\",1}, AbstractArray{var\"#s16\",2}} where var\"#s16\"<:Real}","page":"API","title":"Distributions.logpdf","text":"logpdf(f::FiniteGP, y::AbstractVecOrMat{<:Real})\n\nThe logpdf of y under f if is y isa AbstractVector. logpdf of each column of y if y isa Matrix.\n\njulia> f = GP(Matern32Kernel());\n\njulia> x = randn(11);\n\njulia> y = rand(f(x));\n\njulia> logpdf(f(x), y) isa Real\ntrue\n\njulia> Y = rand(f(x), 3);\n\njulia> logpdf(f(x), Y) isa AbstractVector{<:Real}\ntrue\n\n\n\n\n\n","category":"method"},{"location":"api/#Distributions.logpdf-Tuple{AbstractGPs.LatentFiniteGP,NamedTuple{(:f, :y),T} where T<:Tuple}","page":"API","title":"Distributions.logpdf","text":"logpdf(lfgp::LatentFiniteGP, y::NamedTuple{(:f, :y)})\n\n    log p(y f x)\n\nReturns the joint log density of the gaussian process output f and real output y.\n\n\n\n\n\n","category":"method"},{"location":"api/#Statistics.cov-Tuple{AbstractGPs.AbstractGP,AbstractArray{T,1} where T,AbstractArray{T,1} where T}","page":"API","title":"Statistics.cov","text":"cov(f::AbstractGP, x::AbstractVector, y::AbstractVector)\n\nCompute the length(x) by length(y) cross-covariance matrix between f(x) and f(y).\n\n\n\n\n\n","category":"method"},{"location":"api/#Statistics.cov-Tuple{AbstractGPs.AbstractGP,AbstractArray{T,1} where T}","page":"API","title":"Statistics.cov","text":"cov(f::AbstractGP, x::AbstractVector)\n\nCompute the length(x) by length(x) covariance matrix of the multivariate Normal f(x).\n\n\n\n\n\n","category":"method"},{"location":"api/#Statistics.cov-Tuple{AbstractGPs.FiniteGP,AbstractGPs.FiniteGP}","page":"API","title":"Statistics.cov","text":"cov(fx::FiniteGP, gx::FiniteGP)\n\nCompute the cross-covariance matrix between fx and gx.\n\njulia> f = GP(Matern32Kernel());\n\njulia> x1 = randn(11);\n\njulia> x2 = randn(13);\n\njulia> cov(f(x1), f(x2)) == kernelmatrix(Matern32Kernel(), x1, x2)\ntrue\n\n\n\n\n\n","category":"method"},{"location":"api/#Statistics.cov-Tuple{AbstractGPs.FiniteGP}","page":"API","title":"Statistics.cov","text":"cov(f::FiniteGP)\n\nCompute the covariance matrix of fx.\n\nNoise-free observations\n\njulia> f = GP(Matern52Kernel());\n\njulia> x = randn(11);\n\njulia> cov(f(x)) == kernelmatrix(Matern52Kernel(), x)\ntrue\n\nIsotropic observation noise\n\njulia> cov(f(x, 0.1)) == kernelmatrix(Matern52Kernel(), x) + 0.1 * I\ntrue\n\nIndependent anisotropic observation noise\n\njulia> s = rand(11);\n\njulia> cov(f(x, s)) == kernelmatrix(Matern52Kernel(), x) + Diagonal(s)\ntrue\n\nCorrelated observation noise\n\njulia> A = randn(11, 11); S = A'A;\n\njulia> cov(f(x, S)) == kernelmatrix(Matern52Kernel(), x) + S\ntrue\n\n\n\n\n\n","category":"method"},{"location":"api/#Statistics.mean-Tuple{AbstractGPs.AbstractGP,AbstractArray{T,1} where T}","page":"API","title":"Statistics.mean","text":"mean(f::AbstractGP, x::AbstractVector)\n\nComputes the mean vector of the multivariate Normal f(x).\n\n\n\n\n\n","category":"method"},{"location":"api/#Statistics.mean-Tuple{AbstractGPs.FiniteGP}","page":"API","title":"Statistics.mean","text":"mean(fx::FiniteGP)\n\nCompute the mean vector of fx.\n\njulia> f = GP(Matern52Kernel());\n\njulia> x = randn(11);\n\njulia> mean(f(x)) == zeros(11)\ntrue\n\n\n\n\n\n","category":"method"},{"location":"examples/regression_1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"EditURL = \"https://github.com/JuliaGaussianProcesses/AbstractGPs.jl/blob/master/examples/regression_1d.jl\"","category":"page"},{"location":"examples/regression_1d/#One-dimensional-regression","page":"One-dimensional regression","title":"One-dimensional regression","text":"","category":"section"},{"location":"examples/regression_1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"(Image: ) (Image: )","category":"page"},{"location":"examples/regression_1d/#Setup","page":"One-dimensional regression","title":"Setup","text":"","category":"section"},{"location":"examples/regression_1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"Loading the necessary packages and setting seed.","category":"page"},{"location":"examples/regression_1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"using AbstractGPs\nusing Distributions\nusing StatsFuns\n\nusing Plots\ndefault(legend=:outertopright, size=(700,400))\n\nusing Random\nRandom.seed!(1234)\nnothing #hide","category":"page"},{"location":"examples/regression_1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"Load toy regression dataset taken from GPFlow examples.","category":"page"},{"location":"examples/regression_1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"x = [0.8658165855998895, 0.6661700880180962, 0.8049218148148531, 0.7714303440386239,\n    0.14790478354654835, 0.8666105548197428, 0.007044577166530286, 0.026331737288148638,\n    0.17188596617099916, 0.8897812990554013, 0.24323574561119998, 0.028590102134105955]\ny = [1.5255314337144372, 3.6434202968230003, 3.010885733911661, 3.774442382979625,\n    3.3687639483798324, 1.5506452040608503, 3.790447985799683, 3.8689707574953,\n    3.4933565751758713, 1.4284538820635841, 3.8715350915692364, 3.7045949061144983]\nscatter(x, y; xlabel=\"x\", ylabel=\"y\", legend=false)","category":"page"},{"location":"examples/regression_1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"We split the observations into train and test data.","category":"page"},{"location":"examples/regression_1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"x_train = x[1:8]\ny_train = y[1:8]\nx_test = x[9:end]\ny_test = y[9:end]\nnothing #hide","category":"page"},{"location":"examples/regression_1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"We instantiate a Gaussian process with a Matern kernel. The kernel has fixed variance and length scale parameters of default value 1.","category":"page"},{"location":"examples/regression_1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"f = GP(Matern52Kernel())\nnothing #hide","category":"page"},{"location":"examples/regression_1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"We create a finite dimentional projection at the inputs of the training dataset observed under Gaussian noise with standard deviation sigma = 01, and compute the log-likelihood of the outputs of the training dataset.","category":"page"},{"location":"examples/regression_1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"fx = f(x_train, 0.1)\nlogpdf(fx, y_train)","category":"page"},{"location":"examples/regression_1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"We compute the posterior Gaussian process given the training data, and calculate the log-likelihood of the test dataset.","category":"page"},{"location":"examples/regression_1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"p_fx = posterior(fx, y_train)\nlogpdf(p_fx(x_test), y_test)","category":"page"},{"location":"examples/regression_1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"We plot the posterior Gaussian process along with the observations.","category":"page"},{"location":"examples/regression_1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"scatter(\n    x_train, y_train;\n    xlim=(0,1), xlabel=\"x\", ylabel=\"y\",\n    title=\"posterior (default parameters)\", label=\"Train Data\",\n)\nscatter!(x_test, y_test; label=\"Test Data\")\nplot!(p_fx, 0:0.001:1; label=false)","category":"page"},{"location":"examples/regression_1d/#Markov-Chain-Monte-Carlo","page":"One-dimensional regression","title":"Markov Chain Monte Carlo","text":"","category":"section"},{"location":"examples/regression_1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"Previously we computed the log likelihood of the untuned kernel parameters of the GP. We now also perform approximate inference over said kernel parameters using different Markov chain Monte Carlo (MCMC) methods. I.e., we approximate the posterior distribution of the kernel parameters with samples from a Markov chain.","category":"page"},{"location":"examples/regression_1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"We define a function which returns the log-likelihood of the data for different variance and inverse lengthscale parameters of the Matern kernel. We ensure that these parameters are positive with the softplus function","category":"page"},{"location":"examples/regression_1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"f(x) = log (1 + exp x)","category":"page"},{"location":"examples/regression_1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"struct GPLoglikelihood{X,Y}\n    x::X\n    y::Y\nend\n\nfunction (ℓ::GPLoglikelihood)(params)\n    kernel = ScaledKernel(\n        transform(\n            Matern52Kernel(),\n            ScaleTransform(softplus(params[1]))\n        ),\n        softplus(params[2]),\n    )\n    f = GP(kernel)\n    fx = f(ℓ.x, 0.1)\n    return logpdf(fx, ℓ.y)\nend\n\nconst loglik_train = GPLoglikelihood(x_train, y_train)\nnothing #hide","category":"page"},{"location":"examples/regression_1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"We define a Gaussian prior for the joint distribution of the two transformed kernel parameters. We assume that both parameters are independent with mean 0 and variance 1.","category":"page"},{"location":"examples/regression_1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"logprior(params) = logpdf(MvNormal(2, 1), params)\nnothing #hide","category":"page"},{"location":"examples/regression_1d/#Hamiltonian-Monte-Carlo","page":"One-dimensional regression","title":"Hamiltonian Monte Carlo","text":"","category":"section"},{"location":"examples/regression_1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"We start with a Hamiltonian Monte Carlo (HMC) sampler. More precisely, we use the No-U-Turn sampler (NUTS), which is provided by the Julia packages AdvancedHMC.jl and DynamicHMC.jl.","category":"page"},{"location":"examples/regression_1d/#AdvancedHMC","page":"One-dimensional regression","title":"AdvancedHMC","text":"","category":"section"},{"location":"examples/regression_1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"We start with performing inference with AdvancedHMC.","category":"page"},{"location":"examples/regression_1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"using AdvancedHMC\nusing ForwardDiff","category":"page"},{"location":"examples/regression_1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"Set the number of samples to draw and warmup iterations.","category":"page"},{"location":"examples/regression_1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"n_samples = 2_000\nn_adapts = 1_000\nnothing #hide","category":"page"},{"location":"examples/regression_1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"Define a Hamiltonian system of the log joint probability.","category":"page"},{"location":"examples/regression_1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"logjoint_train(params) = loglik_train(params) + logprior(params)\nmetric = DiagEuclideanMetric(2)\nhamiltonian = Hamiltonian(metric, logjoint_train, ForwardDiff)\nnothing #hide","category":"page"},{"location":"examples/regression_1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"Define a leapfrog solver, with initial step size chosen heuristically.","category":"page"},{"location":"examples/regression_1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"initial_params = rand(2)\ninitial_ϵ = find_good_stepsize(hamiltonian, initial_params)\nintegrator = Leapfrog(initial_ϵ)\nnothing #hide","category":"page"},{"location":"examples/regression_1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"Define an HMC sampler, with the following components:","category":"page"},{"location":"examples/regression_1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"multinomial sampling scheme,\ngeneralised No-U-Turn criteria, and\nwindowed adaption for step-size and diagonal mass matrix","category":"page"},{"location":"examples/regression_1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"proposal = NUTS{MultinomialTS, GeneralisedNoUTurn}(integrator)\nadaptor = StanHMCAdaptor(MassMatrixAdaptor(metric), StepSizeAdaptor(0.8, integrator))\nnothing #hide","category":"page"},{"location":"examples/regression_1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"We draw samples from the posterior distribution of kernel parameters. These samples are in the unconstrained space mathbbR^2.","category":"page"},{"location":"examples/regression_1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"samples, _ = sample(\n    hamiltonian,\n    proposal,\n    initial_params,\n    n_samples,\n    adaptor,\n    n_adapts;\n    progress=false,\n)\nnothing #hide","category":"page"},{"location":"examples/regression_1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"We transform the samples back to the constrained space and compute the mean of both parameters:","category":"page"},{"location":"examples/regression_1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"samples_constrained = [map(softplus, p) for p in samples]\nmean_samples = mean(samples_constrained)","category":"page"},{"location":"examples/regression_1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"We plot a histogram of the samples for the two parameters. The vertical line in each graph indicates the mean of the samples.","category":"page"},{"location":"examples/regression_1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"histogram(\n    reduce(hcat, samples_constrained)';\n    xlabel=\"sample\", ylabel=\"counts\", layout=2,\n    title=[\"inverse length scale\" \"variance\"], legend=false,\n)\nvline!(mean_samples'; linewidth=2)","category":"page"},{"location":"examples/regression_1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"We approximate the log-likelihood of the test data using the posterior Gaussian processes for kernels with the sampled kernel parameters. We can observe that there is a significant improvement over the log-likelihood of the test data with respect to the posterior Gaussian process with default kernel parameters of value 1.","category":"page"},{"location":"examples/regression_1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"struct GPPosterior{X,Y}\n    x::X\n    y::Y\nend\n\nfunction (g::GPPosterior)(p)\n    kernel = ScaledKernel(\n        transform(\n            Matern52Kernel(),\n            ScaleTransform(softplus(p[1]))\n        ),\n        softplus(p[2]),\n    )\n    f = GP(kernel)\n    return posterior(f(g.x, 0.1), g.y)\nend\n\nconst gp_posterior = GPPosterior(x_train, y_train)\n\nmean(logpdf(gp_posterior(p)(x_test), y_test) for p in samples)","category":"page"},{"location":"examples/regression_1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"We sample a function from the posterior GP for the final 100 samples of kernel parameters.","category":"page"},{"location":"examples/regression_1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"plt = scatter(\n    x_train, y_train;\n    xlim=(0,1), xlabel=\"x\", ylabel=\"y\",\n    title=\"posterior (AdvancedHMC)\", label=\"Train Data\",\n)\nscatter!(plt, x_test, y_test; label=\"Test Data\")\nfor p in samples[(end-100):end]\n    sampleplot!(plt, gp_posterior(p)(0:0.02:1), 1)\nend\nplt","category":"page"},{"location":"examples/regression_1d/#DynamicHMC","page":"One-dimensional regression","title":"DynamicHMC","text":"","category":"section"},{"location":"examples/regression_1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"We repeat the inference with DynamicHMC.","category":"page"},{"location":"examples/regression_1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"using DynamicHMC\nusing LogDensityProblems","category":"page"},{"location":"examples/regression_1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"We have to implement the LogDensityProblems interface for GPLogLikelihood.","category":"page"},{"location":"examples/regression_1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"# Log joint density\nLogDensityProblems.logdensity(ℓ::GPLoglikelihood, params) = ℓ(params) + logprior(params)\n\n# The parameter space is two-dimensional\nLogDensityProblems.dimension(::GPLoglikelihood) = 2\n\n# `GPLoglikelihood` does not allow to evaluate derivatives of\n# the log-likelihood function\nfunction LogDensityProblems.capabilities(::Type{<:GPLoglikelihood})\n    LogDensityProblems.LogDensityOrder{0}()\nend","category":"page"},{"location":"examples/regression_1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"Now we can draw samples from the posterior distribution of kernel parameters with DynamicHMC. Again we use ForwardDiff.jl to compute the derivatives of the log joint density with automatic differentiation.","category":"page"},{"location":"examples/regression_1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"samples = mcmc_with_warmup(\n    Random.GLOBAL_RNG,\n    ADgradient(:ForwardDiff, loglik_train),\n    n_samples;\n    reporter = NoProgressReport(),\n).chain\nnothing #hide","category":"page"},{"location":"examples/regression_1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"We transform the samples back to the constrained space and compute the mean of both parameters:","category":"page"},{"location":"examples/regression_1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"samples_constrained = [map(softplus, p) for p in samples]\nmean_samples = mean(samples_constrained)","category":"page"},{"location":"examples/regression_1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"We plot a histogram of the samples for the two parameters. The vertical line in each graph indicates the mean of the samples.","category":"page"},{"location":"examples/regression_1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"histogram(\n    reduce(hcat, samples_constrained)';\n    xlabel=\"sample\", ylabel=\"counts\", layout=2,\n    title=[\"inverse length scale\" \"variance\"], legend=false,\n)\nvline!(mean_samples'; linewidth=2)","category":"page"},{"location":"examples/regression_1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"Again we can observe that there is a significant improvement over the log-likelihood of the test data with respect to the posterior Gaussian process with default kernel parameters.","category":"page"},{"location":"examples/regression_1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"mean(logpdf(gp_posterior(p)(x_test), y_test) for p in samples)","category":"page"},{"location":"examples/regression_1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"We sample a function from the posterior GP for the final 100 samples of kernel parameters.","category":"page"},{"location":"examples/regression_1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"plt = scatter(\n    x_train, y_train;\n    xlim=(0,1), xlabel=\"x\", ylabel=\"y\",\n    title=\"posterior (DynamicHMC)\", label=\"Train Data\",\n)\nscatter!(plt, x_test, y_test; label=\"Test Data\")\nfor p in samples[(end-100):end]\n    sampleplot!(plt, gp_posterior(p)(0:0.02:1), 1)\nend\nplt","category":"page"},{"location":"examples/regression_1d/#Elliptical-slice-sampling","page":"One-dimensional regression","title":"Elliptical slice sampling","text":"","category":"section"},{"location":"examples/regression_1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"Instead of HMC, we use elliptical slice sampling which is provided by the Julia package EllipticalSliceSampling.jl.","category":"page"},{"location":"examples/regression_1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"using EllipticalSliceSampling","category":"page"},{"location":"examples/regression_1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"We draw 2000 samples from the posterior distribution of kernel parameters.","category":"page"},{"location":"examples/regression_1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"samples = sample(\n    ESSModel(\n        MvNormal(2, 1), # Gaussian prior\n        loglik_train,\n    ),\n    ESS(),\n    n_samples;\n    progress=false,\n)\nnothing #hide","category":"page"},{"location":"examples/regression_1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"We transform the samples back to the constrained space and compute the mean of both parameters:","category":"page"},{"location":"examples/regression_1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"samples_constrained = [map(softplus, p) for p in samples]\nmean_samples = mean(samples_constrained)","category":"page"},{"location":"examples/regression_1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"We plot a histogram of the samples for the two parameters. The vertical line in each graph indicates the mean of the samples.","category":"page"},{"location":"examples/regression_1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"histogram(\n    reduce(hcat, samples_constrained)';\n    xlabel=\"sample\", ylabel=\"counts\", layout=2,\n    title=[\"inverse length scale\" \"variance\"],\n)\nvline!(mean_samples'; layout=2, labels=\"mean\")","category":"page"},{"location":"examples/regression_1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"Again we can observe that there is a significant improvement over the log-likelihood of the test data with respect to the posterior Gaussian process with default kernel parameters.","category":"page"},{"location":"examples/regression_1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"mean(logpdf(gp_posterior(p)(x_test), y_test) for p in samples)","category":"page"},{"location":"examples/regression_1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"We sample a function from the posterior GP for the final 100 samples of kernel parameters.","category":"page"},{"location":"examples/regression_1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"plt = scatter(\n    x_train, y_train;\n    xlim=(0,1), xlabel=\"x\", ylabel=\"y\",\n    title=\"posterior (EllipticalSliceSampling)\", label=\"Train Data\",\n)\nscatter!(plt, x_test, y_test; label=\"Test Data\")\nfor p in samples[(end-100):end]\n    sampleplot!(plt, gp_posterior(p)(0:0.02:1), 1)\nend\nplt","category":"page"},{"location":"examples/regression_1d/#Variational-Inference","page":"One-dimensional regression","title":"Variational Inference","text":"","category":"section"},{"location":"examples/regression_1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"Sanity check for the Evidence Lower BOund (ELBO) implemented according to M. K. Titsias's Variational learning of inducing variables in sparse Gaussian processes.","category":"page"},{"location":"examples/regression_1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"elbo(fx, y_train, f(rand(5)))","category":"page"},{"location":"examples/regression_1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"We use the LBFGS algorithm to maximize the given ELBO. It is provided by the Julia package Optim.jl.","category":"page"},{"location":"examples/regression_1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"using Optim","category":"page"},{"location":"examples/regression_1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"We define a function which returns the negative ELBO for different variance and inverse lengthscale parameters of the Matern kernel and different pseudo-points. We ensure that the kernel parameters are positive with the softplus function","category":"page"},{"location":"examples/regression_1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"f(x) = log (1 + exp x)","category":"page"},{"location":"examples/regression_1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"and that the pseudo-points are in the unit interval 01 with the logistic function","category":"page"},{"location":"examples/regression_1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"f(x) = frac11 + exp(-x)","category":"page"},{"location":"examples/regression_1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"struct NegativeELBO{X,Y}\n    x::X\n    y::Y\nend\n\nfunction (g::NegativeELBO)(params)\n    kernel = ScaledKernel(\n        transform(\n            Matern52Kernel(),\n            ScaleTransform(softplus(params[1]))\n        ),\n        softplus(params[2]),\n    )\n    f = GP(kernel)\n    fx = f(g.x, 0.1)\n    return -elbo(fx, g.y, f(logistic.(params[3:end])))\nend\nnothing #hide","category":"page"},{"location":"examples/regression_1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"We randomly initialize the kernel parameters and 5 pseudo points, and minimize the negative ELBO with the LBFGS algorithm and obtain the following optimal parameters:","category":"page"},{"location":"examples/regression_1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"x0 = rand(7)\nopt = optimize(NegativeELBO(x_train, y_train), x0, LBFGS())","category":"page"},{"location":"examples/regression_1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"opt.minimizer","category":"page"},{"location":"examples/regression_1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"The optimized value of the inverse lengthscale is","category":"page"},{"location":"examples/regression_1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"softplus(opt.minimizer[1])","category":"page"},{"location":"examples/regression_1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"and of the variance is","category":"page"},{"location":"examples/regression_1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"softplus(opt.minimizer[2])","category":"page"},{"location":"examples/regression_1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"We compute the log-likelihood of the test data for the resulting approximate posterior. We can observe that there is a significant improvement over the log-likelihood with the default kernel parameters of value 1.","category":"page"},{"location":"examples/regression_1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"opt_kernel = ScaledKernel(\n    transform(\n        Matern52Kernel(),\n        ScaleTransform(softplus(opt.minimizer[1]))\n    ),\n    softplus(opt.minimizer[2]),\n)\nopt_f = GP(opt_kernel)\nopt_fx = opt_f(x_train, 0.1)\nap = approx_posterior(VFE(), opt_fx, y_train, opt_f(logistic.(opt.minimizer[3:end])))\nlogpdf(ap(x_test), y_test)","category":"page"},{"location":"examples/regression_1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"We visualize the approximate posterior with optimized parameters.","category":"page"},{"location":"examples/regression_1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"scatter(\n    x_train, y_train;\n    xlim=(0,1), xlabel=\"x\", ylabel=\"y\",\n    title=\"posterior (VI with sparse grid)\", label=\"Train Data\",\n)\nscatter!(x_test, y_test; label=\"Test Data\")\nplot!(ap, 0:0.001:1; label=false)\nvline!(logistic.(opt.minimizer[3:end]); label=\"Pseudo-points\")","category":"page"},{"location":"examples/regression_1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"","category":"page"},{"location":"examples/regression_1d/","page":"One-dimensional regression","title":"One-dimensional regression","text":"This page was generated using Literate.jl.","category":"page"},{"location":"#AbstractGPs.jl","page":"Home","title":"AbstractGPs.jl","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Abstract types and methods for Gaussian Processes.","category":"page"},{"location":"","page":"Home","title":"Home","text":"AbstractGPs.jl is a package that defines a low-level API for working with Gaussian processes (GPs), and basic functionality for working with them in the simplest cases. As such it is aimed more at developers and researchers who are interested in using it as a building block than end-users of GPs.","category":"page"},{"location":"#Features","page":"Home","title":"Features","text":"","category":"section"},{"location":"#Setup","page":"Home","title":"Setup","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"using AbstractGPs, Random\nrng = MersenneTwister(0)\n\n# Construct a zero-mean Gaussian process with a matern-3/2 kernel.\nf = GP(Matern32Kernel())\n\n# Specify some input and target locations.\nx = randn(rng, 10)\ny = randn(rng, 10)","category":"page"},{"location":"#Finite-dimensional-projection","page":"Home","title":"Finite dimensional projection","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Look at the finite-dimensional projection of f at x, under zero-mean observation noise with variance 0.1.","category":"page"},{"location":"","page":"Home","title":"Home","text":"fx = f(x, 0.1)","category":"page"},{"location":"#Sample-from-GP-from-the-prior-at-x-under-noise.","page":"Home","title":"Sample from GP from the prior at x under noise.","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"y_sampled = rand(rng, fx)","category":"page"},{"location":"#Compute-the-log-marginal-probability-of-y.","page":"Home","title":"Compute the log marginal probability of y.","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"logpdf(fx, y)","category":"page"},{"location":"#Construct-the-posterior-process-implied-by-conditioning-f-at-x-on-y.","page":"Home","title":"Construct the posterior process implied by conditioning f at x on y.","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"f_posterior = posterior(fx, y)","category":"page"},{"location":"#A-posterior-process-follows-the-AbstractGP-interface,-so-the-same-functions-which-work-on-the-posterior-as-on-the-prior.","page":"Home","title":"A posterior process follows the AbstractGP interface, so the same functions which work on the posterior as on the prior.","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"rand(rng, f_posterior(x))\nlogpdf(f_posterior(x), y)","category":"page"},{"location":"#Compute-the-VFE-approximation-to-the-log-marginal-probability-of-y.","page":"Home","title":"Compute the VFE approximation to the log marginal probability of y.","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Here, z is a set of pseudo-points. ","category":"page"},{"location":"","page":"Home","title":"Home","text":"z = randn(rng, 4)\nu = f(z)","category":"page"},{"location":"#Evidence-Lower-BOund-(ELBO)","page":"Home","title":"Evidence Lower BOund (ELBO)","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"We provide a ready implentation of elbo w.r.t to the pseudo points. We can perform Variational Inference on pseudo-points by maximizing the ELBO term w.r.t pseudo-points z and any kernel parameters. For more information, see examples. ","category":"page"},{"location":"","page":"Home","title":"Home","text":"elbo(fx, y, u)","category":"page"},{"location":"#Construct-the-approximate-posterior-process-implied-by-the-VFE-approximation.","page":"Home","title":"Construct the approximate posterior process implied by the VFE approximation.","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"The optimal pseudo-points obtained above can be used to create a approximate/sparse posterior. This can be used like a regular posterior in many cases.","category":"page"},{"location":"","page":"Home","title":"Home","text":"f_approx_posterior = approx_posterior(VFE(), fx, y, u)","category":"page"},{"location":"#An-approximate-posterior-process-is-yet-another-AbstractGP,-so-you-can-do-things-with-it-like","page":"Home","title":"An approximate posterior process is yet another AbstractGP, so you can do things with it like","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"marginals(f_approx_posterior(x))","category":"page"},{"location":"#Sequential-Conditioning","page":"Home","title":"Sequential Conditioning","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Sequential conditioning allows you to compute your posterior in an online fashion. We do this in an efficient manner by updating the cholesky factorisation of the covariance matrix and avoiding recomputing it from original covariance matrix.","category":"page"},{"location":"","page":"Home","title":"Home","text":"# Define GP prior\nf = GP(SqExponentialKernel())","category":"page"},{"location":"#Exact-Posterior","page":"Home","title":"Exact Posterior","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"# Generate posterior with the first batch of data on the prior f1.\np_fx = posterior(f(x[1:3], 0.1), y[1:3])\n\n# Generate posterior with the second batch of data considering posterior p_fx1 as the prior.\np_p_fx = posterior(p_fx(x[4:10], 0.1), y[4:10])","category":"page"},{"location":"#Approximate-Posterior","page":"Home","title":"Approximate Posterior","text":"","category":"section"},{"location":"#Adding-observations-in-an-sequential-fashion","page":"Home","title":"Adding observations in an sequential fashion","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Z1 = rand(rng, 4)\nZ2 = rand(rng, 3)\np_fx = approx_posterior(VFE(), f(x[1:7], 0.1), y[1:7], f(Z))\nu_p_fx = update_approx_posterior(p_fx1, f(x[8:10], 0.1), y[8:10])","category":"page"},{"location":"#Adding-pseudo-points-in-an-sequential-fashion","page":"Home","title":"Adding pseudo-points in an sequential fashion","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"\np_fx1 = approx_posterior(VFE(), f(X, 0.1), y, f(Z1))\nu_p_fx1 = update_approx_posterior(p_fx1, f(Z2))","category":"page"},{"location":"#Index","page":"Home","title":"Index","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"","category":"page"}]
}
